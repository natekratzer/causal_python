{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Collider Bias and Women's Wages\"\n",
    "author: \"Nate Kratzer\"\n",
    "date: \"2022-03-12\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://xkcd.com/2560/](xkcd_confounding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Human beings think in terms of stories and in terms of how the actions they take impact the things around them. It's our natural default way of thinking, and generally it's pretty useful. \n",
    "\n",
    "Doing data analysis doesn't stop us from thinking in terms of stories and causation, but it should make us careful. With the increase in data and in the computing power to process it all, there have been claims that all we need in order to understand and act in the world is to listen to the data. But data does not speak for itself! It is interpretted by humans who will interpret it through the lens of causality. \n",
    "\n",
    "This is an introduction to thinking about causal models for data analysis. The purpose is to demonstrate that the popular approach of simply gathering as much data as you can and controlling for it via regression or other methods is not a good one, and is actively misleading in many cases. We should instead carefully think about plausible causal models using tools like diagrams (directed acyclic graphs, or DAGs) and then do data analysis in accordance with those models. \n",
    "\n",
    "## A Simple Example of Confounding\n",
    "\n",
    "Let's start with an example where using regression does make sense. I have noticed that the sports teams I like are more likely to lose when I am watching them on TV. This is true, but the idea that my watching them causes them to lose is not plausible. So either I'm mistaken in my data collection, very unlucky in my fanship (I am a fan of Cleveland sports teams, so this does seem likely), or there's something else that explains the connection between my watching and my team losing. We can draw a simple diagram of what we've observed so far. \n",
    "\n",
    "(I'm using [Mermaid](https://mermaid.live/edit) and will put the code for each diagram above them so that it's easy to recreate and edit later).\n",
    "\n",
    "graph LR;\n",
    "    A[Watch Game]-->B[Lose Game]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](watch_and_lose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The games I choose to watch are not random, I don't watch every game my teams play, and I'm more likely to watch big games where they're playing a quality opponent. That should also have an impact on how likely they are to win the game.\n",
    "\n",
    "graph LR;\n",
    "    A[Good Opponent]-->B[Watch Game]\n",
    "    A[Good Opponent]-->C[Lose Game]\n",
    "    B==>C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](opp_quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know what the proper causal model looks like, we can see that the conclusion that my watching games caused my teams to lose was based on an incomplete view - or more technically it suffered from omitted variable bias. The analysis left out an important variable that impacted things. Once we control for opponent quality, the relationship between my watching and my team losing should go back to zero.\n",
    "\n",
    "## A Much More Important Example: Women's Wages\n",
    "\n",
    "The idea of drawing out the diagram before doing the analysis can be applied to more important cases, like the ongoing dispute around the wage gap between men and women. Here, I'm taking an example from the excellent book _Causal Inference: The Mixtape_ by Scott Cunningham. \n",
    "\n",
    "When companies are accused of paying women less one of their first lines of defense is to argue that if you account for the occupational differences within the company between men and women the wage gap vanishes or at least shrinks dramatically. Cunningham (and I) think this is a poor causal model and an inadequate defense. This is important, so we're going to consider several causal models and look directly at what they tell us using some simulated data under different specifications. Using simulated data gives us the advantage of knowing the truth of the data - so to speak - we'll create it to have certain causal relationships and then we'll see how the different models capture (and fail to capture) those relationships.\n",
    "\n",
    "I'll start with the causal diagram that we're going to use to simulate our data. It's a bit complicated, but we'll take it piece by piece as we move through the data simulation and modeling.\n",
    "\n",
    "graph LR;\n",
    "    D[Discrimination] --> E[Earnings]\n",
    "    D --> O[Occupation]\n",
    "    O --> E\n",
    "    F[Female] --> D\n",
    "    A[Ability] -.-> O\n",
    "    A -.-> E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gd_dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for generating arrays with random numbers\n",
    "import pandas as pd # dataframes\n",
    "import statsmodels.api as sm # to run the actual ols model\n",
    "\n",
    "np.random.seed(42) # to make it reproducible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first generate a labor force that is half female and has ability randomly distributed. In the causal model sketched above both Female and Ability are root causes - they're not caused by anything else. So that's the place we'll start.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = {\n",
    "    'female'  : np.random.randint(low = 0, high = 2, size = 10000, dtype = int), #the high argument is not inclusive, so this is randomly generating 0s and 1s. \n",
    "    'ability' : np.random.normal(size = 10000),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data = generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate some other variables of interest. We're looking at the impact of discrimination, so let's set that to be experienced by the female half of the labor force that we've simulated. We're going to assume that discrimination affects both wages and choice of occupation. Here we're worried about occupations in terms of higher and lower pay scales, so let's set occupations to be positively associated with ability and negatively associated with discrimination. \n",
    "\n",
    "Finally, wages are negatively associated with discrimination and positively associated with both occupation and ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>ability</th>\n",
       "      <th>discrimination</th>\n",
       "      <th>occupation</th>\n",
       "      <th>wage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.498700</td>\n",
       "      <td>-0.008041</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>-0.009388</td>\n",
       "      <td>0.471065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500023</td>\n",
       "      <td>1.004178</td>\n",
       "      <td>0.500023</td>\n",
       "      <td>2.449597</td>\n",
       "      <td>4.545405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.922400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.018905</td>\n",
       "      <td>-18.328506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.674327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.640437</td>\n",
       "      <td>-2.517222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.022777</td>\n",
       "      <td>0.482132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.668901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.633467</td>\n",
       "      <td>3.501387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.529055</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.500154</td>\n",
       "      <td>16.731628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             female       ability  discrimination    occupation          wage\n",
       "count  10000.000000  10000.000000    10000.000000  10000.000000  10000.000000\n",
       "mean       0.498700     -0.008041        0.498700     -0.009388      0.471065\n",
       "std        0.500023      1.004178        0.500023      2.449597      4.545405\n",
       "min        0.000000     -3.922400        0.000000    -10.018905    -18.328506\n",
       "25%        0.000000     -0.674327        0.000000     -1.640437     -2.517222\n",
       "50%        0.000000     -0.007682        0.000000     -0.022777      0.482132\n",
       "75%        1.000000      0.668901        1.000000      1.633467      3.501387\n",
       "max        1.000000      3.529055        1.000000      9.500154     16.731628"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['discrimination'] = df['female']\n",
    "df['occupation'] = 1 + 2 * df['ability'] + 0 * df['female'] - 2 * df['discrimination'] + np.random.normal(size = 10000)\n",
    "df['wage'] = 1 - 1 * df['discrimination'] + 1 * df['occupation'] + 2 * df['ability'] + np.random.normal(size = 10000)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our simulated data with specified causal relationships, let's look at a few different regression models. We'll first look at a model that only includes being female as a cause of wages. Here the causal model looks like this:\n",
    "\n",
    "graph LR;\n",
    "    A[Female]-->B[Discrimination]-->C[Wages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](mod1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually don't have direct data on discrimination, so we use data we do observe (being female) in the regression, but the causal assumption is still that the mechanism causing the difference in wages is discrimination. In our simulated data we know there's a 1:1 relationship between being female and being discriminated against, because we set up the discrimination variable that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natek\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>wage</td>       <th>  R-squared:         </th> <td>   0.107</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.107</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1195.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 12 Mar 2022</td> <th>  Prob (F-statistic):</th> <td>2.09e-247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:47:41</td>     <th>  Log-Likelihood:    </th> <td> -28766.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>5.754e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  9998</td>      <th>  BIC:               </th> <td>5.755e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>    1.9522</td> <td>    0.061</td> <td>   32.173</td> <td> 0.000</td> <td>    1.833</td> <td>    2.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>female</th> <td>   -2.9700</td> <td>    0.086</td> <td>  -34.565</td> <td> 0.000</td> <td>   -3.138</td> <td>   -2.802</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.156</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.340</td> <th>  Jarque-Bera (JB):  </th> <td>   2.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.008</td> <th>  Prob(JB):          </th> <td>   0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.071</td> <th>  Cond. No.          </th> <td>    2.62</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   wage   R-squared:                       0.107\n",
       "Model:                            OLS   Adj. R-squared:                  0.107\n",
       "Method:                 Least Squares   F-statistic:                     1195.\n",
       "Date:                Sat, 12 Mar 2022   Prob (F-statistic):          2.09e-247\n",
       "Time:                        09:47:41   Log-Likelihood:                -28766.\n",
       "No. Observations:               10000   AIC:                         5.754e+04\n",
       "Df Residuals:                    9998   BIC:                         5.755e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.9522      0.061     32.173      0.000       1.833       2.071\n",
       "female        -2.9700      0.086    -34.565      0.000      -3.138      -2.802\n",
       "==============================================================================\n",
       "Omnibus:                        2.156   Durbin-Watson:                   2.014\n",
       "Prob(Omnibus):                  0.340   Jarque-Bera (JB):                2.186\n",
       "Skew:                          -0.008   Prob(JB):                        0.335\n",
       "Kurtosis:                       3.071   Cond. No.                         2.62\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up matrices for regression\n",
    "Y = df['wage']\n",
    "X1 = df['female']\n",
    "X2 = df[['female', 'occupation']]\n",
    "X3 = df[['female', 'occupation', 'ability']]\n",
    "\n",
    "# add constants to each X matrix for the intercept of the model\n",
    "X1 = sm.add_constant(X1)\n",
    "X2 = sm.add_constant(X2)\n",
    "X3 = sm.add_constant(X3)\n",
    "\n",
    "model1 = sm.OLS(Y, X1)\n",
    "results1 = model1.fit()\n",
    "results1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a lot here, but what we're mainly interested in is the coefficients, so let's look at those. Here we see that being female has a strong negative impact on wages earned. (Don't worry about the const (constant) term, it's not important in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const     1.952182\n",
       "female   -2.969956\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a surprise based on how we set up the data. It also correctly reflects that in the real world if you just divide wages by gender you will find a large gender gap. \n",
    "\n",
    "The dispute comes in when we talk about controlling for occupation, or a model that looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](control_occ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const         0.208846\n",
       "female        0.559992\n",
       "occupation    1.815929\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = sm.OLS(Y, X2)\n",
    "results2 = model2.fit()\n",
    "\n",
    "model3 = sm.OLS(Y, X3)\n",
    "results3 = model3.fit()\n",
    "\n",
    "results2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks like being female might raise wages slightly. We know that's not right since we know we set up the data to have discrimination based on the way we simulated it. The problem is that when we added occupation to the model we opened up a brand new causal pathway from being female to earnings. It's the one that runs from Female-->Discrimination-->Occupation-->Ability-->Earnings in our original causal model.\n",
    "\n",
    "When we controlled for occupation we did two things:\n",
    "a) Ignored the fact that occupational choice is also a result of discrimination and as a defense of pay discrimination it would then be the mechanism by which discrimination happens, not a defense that discrimination isn't happening.\n",
    "b) Opened up a causal pathway that made our estimates worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gd_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try conditioning on abilty. Here we're back to a clear impact of gender discrimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const         0.988717\n",
       "female       -0.986841\n",
       "occupation    1.025762\n",
       "ability       1.975298\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major problem is that in the real world we can't observe ability directly and put it in a regression model. Another issue is that this causal model is still very incomplete. Nonetheless, the way the sign flips back and forth depending on the model is hopefully an illustration of why it's so important to have a theoretical model and not just throw in as much data as possible.\n",
    "\n",
    "Data is a powerful way to tell stories, but data by itself _never_ tells us everything we need to know. We have to interpret it carefully and think hard about the underlying models of the world we're bringing to the data when we interpret it. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a0dc9486a7cd553ffe67939919d7d1b2e9e40955da6b3e925a89ee921e8fb72"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
